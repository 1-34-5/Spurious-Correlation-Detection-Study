{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53ec828-d605-42e1-a747-8d060256a838",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m CONFIG = full_config[\u001b[33m'\u001b[39m\u001b[33mresnet50\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# or 'deit_small' / 'swin_tiny'\u001b[39;00m\n\u001b[32m     32\u001b[39m device = torch.device(CONFIG.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m train_loader, val_loader, dataset = \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_root\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_metadata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msplit_metadata_path\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_workers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m val_dataset = val_loader.dataset\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Validation dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spurious_correlation/notebooks/../src/data/dataloader.py:38\u001b[39m, in \u001b[36mcreate_dataloaders\u001b[39m\u001b[34m(data_root, split_metadata_path, batch_size, num_workers, img_size, pin_memory)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03mCreate train and validation dataloaders\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m \u001b[33;03m    dataset: Base OxfordIIITPet dataset\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Load base dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dataset = \u001b[43mOxfordIIITPet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrainval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_types\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msegmentation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Load split indices\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(split_metadata_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spurious_correlation/.venv/lib/python3.13/site-packages/torchvision/datasets/oxford_iiit_pet.py:70\u001b[39m, in \u001b[36mOxfordIIITPet.__init__\u001b[39m\u001b[34m(self, root, split, target_types, transforms, transform, target_transform, download)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._download()\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_exists():\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m image_ids = []\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m._labels = []\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup with your existing structure\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.data import create_dataloaders, denormalize\n",
    "\n",
    "# Attribution imports\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "from captum.attr import Saliency, IntegratedGradients, Occlusion\n",
    "\n",
    "# Load config\n",
    "# Load config\n",
    "with open('../experiments/configs/baseline_config.json', 'r') as f:\n",
    "    full_config = json.load(f)\n",
    "\n",
    "CONFIG = full_config['resnet50']  # or 'deit_small' / 'swin_tiny'\n",
    "\n",
    "device = torch.device(CONFIG.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "train_loader, val_loader, dataset = create_dataloaders(\n",
    "    data_root=CONFIG['data']['data_root'],\n",
    "    split_metadata_path=CONFIG['data']['split_metadata_path'],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    num_workers=CONFIG['training']['num_workers'],\n",
    "    img_size=CONFIG['data']['image_size'],\n",
    "    pin_memory=False\n",
    ")\n",
    "val_dataset = val_loader.dataset\n",
    "\n",
    "print(f\"✓ Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"✓ Device: {device}\")\n",
    "print(f\"✓ Num classes: {CONFIG['num_classes']}\")\n",
    "\n",
    "# Create output directories\n",
    "output_base = '../experiments/results/attribution_analysis'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Output directory: {output_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba863eaf-9f25-45ff-a479-c656b5931a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
